{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import _hash_file\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags are read into pandas' DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = pd.read_csv('./opp_data/atti/tags.csv')\n",
    "tagging_df = pd.read_csv('./opp_data/atti/atti_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "Some functions are defined, in order to handle documents and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_tags(act_id: int) -> list:\n",
    "    \"\"\"Return all tags for the act having given id, as triplets:\n",
    "      0 - the tag id,\n",
    "      1 - the tag text\n",
    "      2 - the tag type (teseo|geoteseo|op_geo)\n",
    "    \"\"\"\n",
    "    tags_list = list(tagging_df[tagging_df.act_id==act_id].tags_ids)\n",
    "    if len(tags_list):\n",
    "        tags_list = tags_list[0].split(':')[1:]\n",
    "    return [\n",
    "        (\n",
    "            tags_df[tags_df.id==int(t)].iloc[0]['id'], \n",
    "            tags_df[tags_df.id==int(t)].iloc[0]['name'], \n",
    "            tags_df[tags_df.id==int(t)].iloc[0]['type']\n",
    "        ) for t in tags_list\n",
    "    ]\n",
    "\n",
    "def get_act_tags_ids(act_id: int) -> list:\n",
    "    \"\"\"Return all tags ids, for non-geo tags\"\"\"\n",
    "    return [t[0] for t in get_act_tags(act_id) if 'geo' not in t[2]]\n",
    "\n",
    "def extract_text_from_html(html: string) -> string:\n",
    "    \"\"\"Extract text from act's HTML content, \n",
    "       removing names of MPs that signed the act\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    stripped = soup.get_text().replace(\"\\n\", \" \").replace(\"  \", \" \").strip(' ')\n",
    "    text = re.sub(r'(\\([\\d\\-]*\\)) «(.*)»\\.', '', stripped).strip()\n",
    "    return text\n",
    "\n",
    "def nltk_process(text: string) -> list:\n",
    "    \"\"\"Process a text with NLTK doing the following:\n",
    "    - splitting into words,\n",
    "    - converting to lowe case\n",
    "    - remove punctuation\n",
    "    - filter out stop words\n",
    "    \n",
    "    Returns a list of stemmed words\n",
    "    \"\"\"\n",
    "\n",
    "    # split into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation)) # remove punctuation from each word\n",
    "    stripped = [re_punc.sub('', w) for w in tokens]\n",
    "\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('italian'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    # stemming of words\n",
    "    # stemmer = SnowballStemmer(\"italian\")\n",
    "    # stemmed = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "def preprocess_and_save_docs(\n",
    "    source_zip: string, dest_name: string, \n",
    "    n_docs: int = 100,\n",
    "    cache_path='./datasets',\n",
    "    aws_bucket='opp-datasets',\n",
    "    aws_region='eu-central-1'\n",
    "):\n",
    "    \"\"\"Pre-process ``n_docs`` random documents out of the ones contained in `source_zip` file, \n",
    "    pre-cleaning them for serialization onto disk, along with labels (tags).\n",
    "    \n",
    "    Data are serialized in the ``dest_npz`` file, with the `.npz` compressed format.\n",
    "    A vocabulary is serialized in a json file, with thename extracted from dest_npz.\n",
    "\n",
    "    For each one of the n_docs documents in the zipped file:\n",
    "    - html is parsed and text content is extracted (beautifulsoup)\n",
    "    - both the data list and the vocab Counter are updated\n",
    "    \n",
    "    :return: a tuple with the MD5 hashes of the npz and json files, respectively\n",
    "    \n",
    "    The following data are persisted in ``dest_npz``, using the ``.npz`` format:\n",
    "      ids:    list of original openparlamento ID (to refer to the original ACT)\n",
    "      data:   list of original texts (pre-cleaned)\n",
    "      labels: list of the assigned labels \n",
    "              each label is a triple: (ID, name, type)\n",
    "      vocab:  the complete vocabulary, with occurence counts for each word\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    vocab = Counter()\n",
    "\n",
    "    with open(source_zip, 'rb') as tz:\n",
    "        z = zipfile.ZipFile(tz)\n",
    "        filelist = random.choices(z.filelist, k=n_docs)\n",
    "        for fl in tqdm(filelist):\n",
    "            zf = z.open(fl.filename)\n",
    "            original_html = zf.read()\n",
    "            text_content = extract_text_from_html(original_html)\n",
    "            words = nltk_process(text_content)            \n",
    "            zf.close()\n",
    "            id = int(fl.filename.split('_')[0])\n",
    "            data_f = {\n",
    "                'id': id,\n",
    "                'text_content': text_content,\n",
    "                'tags': get_act_tags(id),\n",
    "            }\n",
    "            data.append(data_f)\n",
    "            vocab.update(words)\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        region_name=aws_region\n",
    "    )\n",
    "    dest_npz = f\"{cache_path}/{dest_name}.npz\"\n",
    "    dest_vocab = f\"{cache_path}/{dest_name}_vocab.json\"\n",
    "\n",
    "    print(f\"Saving data into {dest_npz}\")\n",
    "    np.savez_compressed(\n",
    "        dest_npz, \n",
    "        ids=[doc['id'] for doc in data], \n",
    "        texts=[doc['text_content'] for doc in data], \n",
    "        labels=[doc['tags'] for doc in data], \n",
    "    )\n",
    "    print(f\"Uploading to s3://{aws_bucket}/{dest_name}.npz\")\n",
    "    s3.upload_file(\n",
    "        dest_npz, aws_bucket, f\"{dest_name}.npz\",\n",
    "        ExtraArgs={'ACL': 'public-read'}\n",
    "    )\n",
    "    \n",
    "    print(f\"Saving vocab into {dest_vocab}\")\n",
    "    with open(dest_vocab, 'w') as outfile:\n",
    "        json.dump(vocab, outfile)\n",
    "    print(f\"Uploading to s3://{aws_bucket}/{dest_name}_vocab.json\")\n",
    "    s3.upload_file(\n",
    "        dest_vocab, aws_bucket, f\"{dest_name}_vocab.json\",\n",
    "        ExtraArgs={'ACL': 'public-read'}\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        (f\"https://{aws_bucket}.s3.{aws_region}.amazonaws.com/{dest_name}.npz\", \n",
    "         _hash_file(dest_npz, algorithm='md5')), \n",
    "        (f\"https://{aws_bucket}.s3.{aws_region}.amazonaws.com/{dest_name}_vocab.json\", \n",
    "         _hash_file(dest_vocab, algorithm='md5'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents pre-processing and saving\n",
    "``n_docs`` documents, randomly extracted from the zip file, are pre-processed, saved locally (in the cache) and uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data into ./datasets/tagged_acts_16.npz\n",
      "Uploading to s3://opp-datasets/tagged_acts_16.npz\n",
      "Saving vocab into ./datasets/tagged_acts_16_vocab.json\n",
      "Uploading to s3://opp-datasets/tagged_acts_16_vocab.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('https://opp-datasets.s3.eu-central-1.amazonaws.com/tagged_acts_16.npz',\n",
       "  '6bbcb04434449fad1581aeee2e299698'),\n",
       " ('https://opp-datasets.s3.eu-central-1.amazonaws.com/tagged_acts_16_vocab.json',\n",
       "  '7cdab8ecd73d861b5f8e550c6a9e845e'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_and_save_docs('./opp_data/atti/testi.zip', 'tagged_acts_16', n_docs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing .npz loading and usage\n",
    "The ``.npz`` file is loaded and a random item is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from op.datasets import opp_tagged_acts\n",
    "\n",
    "(train_texts_raw, train_labels_raw), (test_texts_raw, test_labels_raw) = opp_tagged_acts.load_data_16()\n",
    "vocab = opp_tagged_acts.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Atto Camera Ordine del Giorno 9/05389/153presentato daRUBINATO Simonettatesto diMartedì 7 agosto 2012, seduta n. 678 La Camera, premesso che: il provvedimento in esame ha un contenuto estremamente vasto e complesso con norme orientate a favorire la riduzione della spesa pubblica; vi è straordinaria necessità e urgenza di provvedere a dare effettività alle norme in materia di limite massimo retributivo per emolumenti o retribuzioni nell'ambito di rapporti di lavoro dipendente o autonomo con le pubbliche amministrazioni statali, e, in generale, di limite al trattamento economico annuo onnicomprensivo di chiunque riceva a carico delle finanze pubbliche emolumenti o retribuzioni nell'ambito di rapporti di lavoro dipendente o autonomo, ivi incluso il personale in regime di diritto pubblico di cui all'articolo 3 del decreto legislativo 30 marzo 2001, n. 165, stabilendo come parametro massimo di riferimento il trattamento economico del primo presidente della Corte di cassazione; a norma dell'articolo 23-ter del decreto-legge 6 dicembre 2011 n. 201 («Disposizioni urgenti per la crescita, l'equità e il consolidamento dei conti pubblici»), convertito, con modificazioni, dalla legge 22 dicembre 2011 n. 214, nel trattamento economico annuo onnicomprensivo di chiunque riceva a carico delle finanze pubbliche emolumenti o retribuzioni nell'ambito di rapporti di lavoro dipendente o autonomo devono essere computate in modo cumulativo le somme comunque erogate all'interessato, anche nel caso di pluralità di incarichi conferiti da uno stesso organismo nel corso dell'anno o da più organismi, qualora tali emolumenti o retribuzioni siano comunque a carico delle finanze pubbliche; occorre dare effettività a tali norme anche applicando tali disposizioni al personale chiamato, conservando il trattamento economico riconosciuto dall'amministrazione di appartenenza, all'esercizio di funzioni direttive, dirigenziali o equiparate, anche in posizione di fuori ruolo o di aspettativa, presso Ministeri o enti pubblici nazionali, comprese le autorità amministrative indipendenti, impegna il Governo: a valutare l'opportunità, nel rispetto degli equilibri di finanza pubblica, di prevedere con atto avente forza di legge, che il trattamento retributivo percepito annualmente, comprese le indennità e le voci accessorie nonché le eventuali remunerazioni per incarichi ulteriori o consulenze conferiti da amministrazioni pubbliche diverse da quella di appartenenza, dei soggetti che ricevano retribuzioni o emolumenti a carico delle pubbliche finanze in ragione di un rapporto di lavoro subordinato o autonomo nonché di quelli in regime di diritto pubblico di cui all'articolo 3 del decreto legislativo 30 marzo 2001, n. 165, non possa superare il trattamento economico annuale complessivo spettante per la carica al Primo Presidente della Corte di cassazione, pari nell'anno 2011 a euro 293.658,95; a prevedere che tale disposizione di legge si applichi solo qualora il trattamento sia superiore, che in tal caso si riduce al predetto limite e ad escludere, in ogni caso, che trattamenti inferiori possano essere elevati a tale limite a decorrere dalla data di entrata in vigore della presente legge; ad escludere, dal predetto limite, deroghe motivate per le posizioni apicali delle rispettive amministrazioni; a prevedere un limite massimo per i rimborsi spese, che devono essere comunque documentati e motivati. 9/5389/153. (Testo modificato nel corso della seduta)\\xa0Rubinato.\",\n",
       " [(7365, 'consulenze e incarichi per la Pubblica Amministrazione', 'user'),\n",
       "  (1298, 'PUBBLICA AMMINISTRAZIONE', 'teseo'),\n",
       "  (640, 'SPESA PUBBLICA', 'teseo'),\n",
       "  (1562, 'ECONOMIA', 'teseo')])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(train_texts_raw)))\n",
    "train_raw[i], train_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 535),\n",
       " ('comma', 511),\n",
       " ('legge', 457),\n",
       " ('stato', 428),\n",
       " ('essere', 297),\n",
       " ('ministro', 284),\n",
       " ('ministero', 242),\n",
       " ('decreto', 218),\n",
       " ('caso', 216),\n",
       " ('tale', 207)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top n_most_common terms are removed from the vocab and the words\n",
    "n_most_common = 10\n",
    "most_common = [k for k, v in vocab.most_common(n_most_common)]\n",
    "vocab = dict([(k, c) for k,c in vocab.items() if k not in most_common])\n",
    "for doc in data:\n",
    "    doc['words'] = \" \".join([w for w in doc['words'].split() if w in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "x = tokenizer.texts_to_matrix(lines, mode='binary')\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('./datasets/tagged_acts', x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load('./datasets/tagged_acts.npz', allow_pickle=True)\n",
    "xs = loaded['x']\n",
    "labels = loaded['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_hash_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-f42818c37ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_hash_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/tagged_acts.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'md5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_hash_file' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'db3cfb1c873a3c4cb2fe3022f3e68e3e'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_hash_file('./datasets/tagged_acts_tags.csv', algorithm='md5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, hashing_trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(set(text_to_word_sequence(texts[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
